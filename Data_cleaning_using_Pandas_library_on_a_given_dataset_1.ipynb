{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sarupriya1998/Data-Science-projects/blob/Data-cleaning/Data_cleaning_using_Pandas_library_on_a_given_dataset_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ69yhyYePrx"
      },
      "source": [
        "You are given a dataset –“hotel_bookings.csv.”The dataset has a high number of null and  elements that need to be cleansed;  Your job is to create a separate DataFrame with only categorical columns and  perform the  following operations:\n",
        "\n",
        "1.Find the number of null values in each column of the new DataFrame.\n",
        "\n",
        "2.Replace the null values with mode. \n",
        "\n",
        "3.In the \"hotel\"column, replace the hotel names with \"0\" and \"1\" based on the condition that –if,\"hotel\" = \"city_hotel\",then \"hotel\"= \"1\";  else, \"0\".\n",
        "\n",
        "4.Using the label encoder, assign a unique country code to each country.\n",
        "\n",
        "5.Using onehot encoder, encode the month column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPxPFEbOfCOQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df=pd.read_csv(\"hotel_bookings.csv\")\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "folE-Os1sS-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(['company','agent'],axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-TLKIFzAsXlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df=df.select_dtypes(include=['object']).copy()\n",
        "cat_df.head()"
      ],
      "metadata": {
        "id": "f_YlzfPHsgk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cat_df.isnull().sum())"
      ],
      "metadata": {
        "id": "ddGuVV6_s0ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_cat_df=cat_df.fillna(cat_df['country'].mode()[0],inplace=True)\n",
        "print('After replacing null values with mode:')\n",
        "print(cat_df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "TBau3Y3_s6_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df_spf=cat_df.copy()\n",
        "cat_df_spf['hotel']=np.where(cat_df_spf['hotel'].str.contains('City Hotel'),1,0)\n",
        "cat_df_spf.sample(10)"
      ],
      "metadata": {
        "id": "bxHlHOZjtfyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df_spf1=cat_df_spf.copy()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb_make=LabelEncoder()\n",
        "cat_df_spf1['country_code']=lb_make.fit_transform(cat_df_spf['country'])\n",
        "cat_df_spf1.sample(5)\n"
      ],
      "metadata": {
        "id": "qhdeCdAKt4N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_onehot=cat_df_spf.copy()\n",
        "cat_onehot=pd.get_dummies(cat_onehot,columns=['arrival_date_month'])\n",
        "print(cat_onehot.head())"
      ],
      "metadata": {
        "id": "oqYANk8wuh5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2.Create a DataFrame to store exponential data using the NumPy\n",
        "random() function as shown in the dataset section below, and perform\n",
        "the following operations:\n",
        "1. Using the minmax_scaling() function, scale the data between 0 to 1,\n",
        "and plot the original data and scaled data using the Seaborn library\n",
        "2. Using preprocessing.normalize() function, normalize the data, and\n",
        "plot the original data and normalized data using the Seaborn library\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iyGuNFvl2CzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.preprocessing import minmax_scaling\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "4nhFfL6TRbp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(100)\n",
        "data=np.random.exponential(size=1000)\n",
        "df2=pd.DataFrame(data)\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "wKylrFASRxfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_data=minmax_scaling(df2,columns=[0])\n",
        "fig,ax=plt.subplots(1,2)\n",
        "sns.distplot(data,ax=ax[0])\n",
        "ax[0].set_title(\"Original Data\")\n",
        "sns.distplot(scaled_data,ax=ax[1])\n",
        "ax[1].set_title(\"Scaled data\")"
      ],
      "metadata": {
        "id": "NC-PSOZFRWQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "from sklearn import preprocessing\n",
        "normalized_data=preprocessing.normalize(df2)"
      ],
      "metadata": {
        "id": "emqpTTwcUSMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(1,2)\n",
        "sns.distplot(df2[0],figax=ax[0])\n",
        "ax[0].set_title(\"Original Data\")\n",
        "sns.distplot(normalized_data,ax=ax[1])\n",
        "ax[1].set_title('Normalized data')"
      ],
      "metadata": {
        "id": "xlIONHwKUiEp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOr1WBYHuwgGQXdRemlw8td",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}